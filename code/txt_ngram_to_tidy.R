#!/usr/bin/env Rscript

main_process_texts <- function(directory, language, ngram_number, date_hour) {

# Function to remove accent marks from a text
remove_accents <- function(texto) {
  iconv(texto, to = "ASCII//TRANSLIT")
}

# Function to remove numbers from the start and end of the text
# remove_numbers_from_edges <- function(text) {
#   text <- gsub("^\\d+", "", text)  # Remove numbers at the start
#   text <- gsub("\\d+$", "", text)  # Remove numbers at the end
#   return(text)
# }
normalize_document_name <- function(name) {
  name <- tolower(name)
  name <- tools::file_path_sans_ext(name)               # Remove extension
  name <- gsub("[^a-z]", " ", name)                     # Replace non-letters with space
  name <- unlist(strsplit(name, "\\s+"))                # Split by spaces
  name <- name[nchar(name) > 2]                         # Keep words longer than 2 letters
  if (length(name) == 0) return(NA)
  name <- name[which.max(nchar(name))]                  # Keep longest word
  return(name)
}

file_name <- paste0("output/data_table_", date_hour, ".txt")

# Load the table generated by the second script
data_table <- read.table(file_name, sep = "\t", header = FALSE, col.names = c("document", "date"))


# Format the dates in the 'date' column from "dd/mm/yyyy" to "dd-mm-yyyy"
data_table$date <- gsub("/", "-", data_table$date)

# Basename
# data_table$document <- basename(data_table$document)
# data_table$origin_document <- data_table$document
# data_table$document <- sapply(data_table$document, remove_numbers_from_edges)## TEST
data_table$document <- basename(data_table$document)
data_table$origin_document <- data_table$document
data_table$document <- sapply(data_table$document, normalize_document_name)

# Removed tag
data_table$document <- gsub("\\.[^.]+$", "", data_table$document)

infoText <- if (is.numeric(data_table$date[1])) {
  tibble(
    document = character(),
    date = numeric(),
#    document = character(),
    paragraph = numeric(),
#    total_words = numeric(),
    text = character()
  )
} else {
  tibble(
    document = character(),
    date = character(),
#    document = character(),
    paragraph = numeric(),
#    total_words = numeric(),
    text = character()
  )
}

for (i in seq_along(data_table$origin_document)) {
  speech <- readLines(file.path(directory, data_table$origin_document[i]))
  
  # Count the words in each paragraph
  word_counts <- sapply(strsplit(speech, "\\s+"), length)
  
  # Calculate the total words in the document
#  total_words <- sum(word_counts, na.rm = TRUE)
  if (length(speech) == 0) {
    message("⚠️ Archivo vacío: ", file_path)
  }
  
  if (any(is.na(speech))) {
    message("⚠️ Línea NA encontrada en: ", file_path)
  }
  
  temporal <- tibble(document = data_table$document[i],
                     date = data_table$date[i],
#                     document = data_table$document[i],
                     paragraph = seq_along(speech),
#                     total_words = total_words,
                     text = speech)
  
  infoText <- bind_rows(infoText, temporal)
}

infoText <- infoText %>%
  mutate(document = factor(document, levels = unique(document)),
         date = factor(date))

# Read stop words according to the specified language
if (tolower(language) == "sp") {
  stopwords_list <- stopwords(language = "es", source = "stopwords-iso")
} else if (tolower(language) == "en") {
  stopwords_list <- stopwords(language = "en", source = "stopwords-iso")
} else {
  stop("Invalid language argument. Use 'SP' or 'EN'.")
}

# Function to generate tokens and filter ngrams
generate_ngrams <- function(infoText, n) {
  tokens <- infoText %>%
    unnest_tokens(word, text, token = "ngrams", n = n) %>%
    separate(word, into = paste0("word", 1:n), sep = " ") %>%
    filter(across(starts_with("word"), ~ !grepl("\\d+", .) & !(tolower(.) %in% stopwords_list))) %>%
    unite(word, starts_with("word"), sep = " ")
  
  return(tokens)
}

# Generate the ngrams and filter
if (ngram_number == "4") {
  infoText_token <- generate_ngrams(infoText, 4)
} else if (ngram_number == "3") {
  infoText_token <- generate_ngrams(infoText, 3)
} else if (ngram_number == "2") {
  infoText_token <- generate_ngrams(infoText, 2)
} else if (ngram_number == "1" || ngram_number == "") {
  infoText_token <- infoText %>%
    unnest_tokens(word, text) %>%
    filter(!grepl("\\d+", word) & !(tolower(word) %in% stopwords_list))
} else {
  stop("Invalid ngram value. Use 1, 2, or 3.")
}

# Save the result to a file
output_dir <- "output"
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

textWord <- file.path("output/", paste0("words_", date_hour, ".txt"))
write.table(infoText_token, file = textWord, row.names = FALSE, col.names = TRUE, sep = "\t", quote = FALSE)

}
